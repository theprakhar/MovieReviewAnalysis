{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment Analysis\n",
    "## We'll feed the neural network with movie reviews and positive/negative labels and then see if it is able to learn and predict whether a particular review/text is positive or negative\n",
    "### Let us first load the data and store it in lists\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Curating Dataset\n",
    "file=open(\"reviews.txt\",'r')\n",
    "reviews=list(map(lambda x:x[:-1],file.readlines()))\n",
    "file.close()\n",
    "file = open('labels.txt','r') \n",
    "labels = list(map(lambda x:x[:-1].upper(),file.readlines()))\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The neural network\n",
    "## We'll define a class for the network,\n",
    "### - The constructor preprocesses and initializes the data/variables.  \n",
    "### - For reducing the noise in data (in this case), \n",
    "   - We try to eliminate words from our training vocabulary that do not provide much sentiment(or predictive power).\n",
    "     For example,words such as \"the\",\"is\",\"and\",etc.\n",
    "   - We also try to remove words that occur rarely in the reviews,since their quantity is insufficient to train.\n",
    "     \n",
    "   >For these purposes,we use positive to negative ratios,and for that we count the number of times a word appears in both \n",
    " posivite and negative reviews,and store them separately.We also calculate the total count of a word in the reviews.\n",
    " Then if the word occurs more than a particular threshold we calculate its positive to negative ratio.\n",
    " We add words to the vocabulary that occur more than the minimum count,hence eliminating words that are less frequent and if \n",
    " the word is present in our positive to negative ratios we add only those that have value greater than our cutoff,hence \n",
    " eliminating the common words with not much predictive power.\n",
    "   - The input layer only storing whether the word is present or not,and not the number of times it occur. \n",
    "     Since most common words in the review are just \" \",\"is\",etc.,and if we multiply their occurance with the weights,\n",
    "     it is probably a bad idea.\n",
    "     \n",
    "### - For improving the efficiency of the network,\n",
    "\n",
    "   - We take only those words from a particular review for consideration during running/forward propagation that are present in\n",
    "     our vocab.\n",
    "   - for our hidden layer,we consider the weights for only the non-zero items and while training we only update the weights that \n",
    "     were used while propagating forward\n",
    "     \n",
    "   >Hence,reducing the time consumed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from collections import Counter\n",
    "class SentiNetwork:\n",
    "    def __init__(self,reviews,labels,polarity_cutoff = 0.1,min_count = 10,hidden_nodes=10,alpha=0.1):\n",
    "        np.random.seed(1)\n",
    "        self.preprocess(reviews,labels,polarity_cutoff,min_count)\n",
    "        self.inputnodes=len(self.r_vocab)\n",
    "        self.hidden_nodes=hidden_nodes\n",
    "        self.alpha=alpha\n",
    "        self.output_nodes=1\n",
    "        self.w_ih=np.zeros((self.inputnodes,self.hidden_nodes))\n",
    "        self.w_ho=np.random.normal(0.0,self.output_nodes**-0.5,(self.hidden_nodes,self.output_nodes))\n",
    "        self.lay_1=np.zeros((1,self.hidden_nodes))\n",
    "    \n",
    "    def preprocess(self,reviews,labels,polarity_cutoff, min_count):\n",
    "       \n",
    "        \n",
    "        positive_counts = Counter()\n",
    "        negative_counts = Counter()\n",
    "        total_counts = Counter()\n",
    "\n",
    "        for i in range(len(reviews)):\n",
    "            if(labels[i] == 'POSITIVE'):\n",
    "                for word in reviews[i].split(\" \"):\n",
    "                    positive_counts[word] += 1\n",
    "                    total_counts[word] += 1\n",
    "            else:\n",
    "                for word in reviews[i].split(\" \"):\n",
    "                    negative_counts[word] += 1\n",
    "                    total_counts[word] += 1\n",
    "\n",
    "        pos_neg_ratios = Counter()\n",
    "\n",
    "        for term,cnt in list(total_counts.most_common()):\n",
    "            if(cnt >= 50):\n",
    "                pos_neg_ratio = positive_counts[term] / float(negative_counts[term]+1)\n",
    "                pos_neg_ratios[term] = pos_neg_ratio\n",
    "\n",
    "        for word,ratio in pos_neg_ratios.most_common():\n",
    "            if(ratio > 1):\n",
    "                pos_neg_ratios[word] = np.log(ratio)\n",
    "            else:\n",
    "                pos_neg_ratios[word] = -np.log((1 / (ratio + 0.01)))\n",
    "     \n",
    "        review_vocab = set()\n",
    "        for review in reviews:\n",
    "            for word in review.split(\" \"):\n",
    "                if(total_counts[word] > min_count):\n",
    "                    if(word in pos_neg_ratios.keys()):\n",
    "                        if((pos_neg_ratios[word] >= polarity_cutoff) or (pos_neg_ratios[word] <= -polarity_cutoff)):\n",
    "                            review_vocab.add(word)\n",
    "                    else:\n",
    "                        review_vocab.add(word)\n",
    "        self.r_vocab=list(review_vocab)\n",
    "        lvocab=set()\n",
    "        for l in labels:\n",
    "            lvocab.add(l)\n",
    "        self.l_vocab=lvocab\n",
    "        \n",
    "        self.r_vocabsize=len(self.r_vocab)\n",
    "        self.l_vocabsize=len(self.l_vocab)\n",
    "        \n",
    "        self.w2index={} #for storing the index of the word in the vocabulary\n",
    "        for i,w in enumerate(self.r_vocab):\n",
    "            self.w2index[w]=i\n",
    "        self.l2index={}\n",
    "        for i,l in enumerate(self.l_vocab):\n",
    "            self.l2index[l]=i\n",
    "    def update_l1(self,review): #adding only weights of non-zero items\n",
    "        self.lay_1*=0\n",
    "        for ind in review:\n",
    "             self.lay_1+=self.w_ih[ind]\n",
    "    def get_t_l(self,label):\n",
    "        if label==\"POSITIVE\":\n",
    "            return 1\n",
    "        elif label==\"NEGATIVE\":\n",
    "            return 0\n",
    "    def s(self,x):\n",
    "        return 1/(1+np.exp(-x))\n",
    "    def sd(self,o):\n",
    "        return o*(1-o)\n",
    "    def train(self,treviews_r,tlabels):\n",
    "        i=0\n",
    "        treviews=list()\n",
    "        for r in treviews_r:\n",
    "            ind=set()\n",
    "            for w in r.split(\" \"):\n",
    "                if w in self.w2index.keys():\n",
    "                    ind.add(self.w2index[w])\n",
    "            treviews.append(list(ind))\n",
    "        \n",
    "        assert(len(treviews)==len(tlabels))\n",
    "        c=0\n",
    "        for r,l in zip(treviews,tlabels):\n",
    "            \n",
    "            self.update_l1(r)\n",
    "            output=self.s(self.lay_1.dot(self.w_ho))\n",
    "            \n",
    "            error=self.get_t_l(l)-output\n",
    "            errordelta=error*self.sd(output)\n",
    "            errorhid=errordelta.dot(self.w_ho.T)\n",
    "            \n",
    "            \n",
    "            for ind in r:\n",
    "                self.w_ih[ind]+=errorhid[0]*self.alpha #ignoring the x in the equation since x=1\n",
    "            self.w_ho+=self.lay_1.T.dot(errordelta)*self.alpha\n",
    "            \n",
    "            i+=1\n",
    "            if output>=0.5 and l==\"POSITIVE\":\n",
    "                c+=1\n",
    "            elif output<0.5 and l==\"NEGATIVE\":\n",
    "                c+=1\n",
    "            if i>0 and i%(i/10)==0:\n",
    "                print(\"\\rAccuracy : \"+str(c * 100 / float(i+1))[:4]+\" Trained percent : \"+ str(100 * i/float(len(treviews)))[:4],end=\" \")    \n",
    "    \n",
    "                  \n",
    "    def test(self,te_r,te_l):\n",
    "        c,i=0,0\n",
    "        for x,y in zip(te_r,te_l):\n",
    "            pred=self.run(x)\n",
    "            i+=1\n",
    "            if pred==y:\n",
    "                c+=1\n",
    "            if i>0 and i%(i/10)==0:\n",
    "                print(\"\\r Accuracy: \"+str(c * 100 / float(i+1))[:4]+\" Completion % : \"+ str(100 * i/float(len(te_r)))[:4],end=\" \")   \n",
    "    def run(self,review):\n",
    "        self.lay_1*=0\n",
    "        ind=set()\n",
    "        for w in review.lower().split(\" \"):\n",
    "            if w in self.w2index.keys():\n",
    "                ind.add(self.w2index[w])\n",
    "            #output hidden ,since we take 1 if item present ,therefore,considering only those present:\n",
    "        self.update_l1(ind)\n",
    "        output=self.s(self.lay_1.dot(self.w_ho))\n",
    "        if output[0] >= 0.5:\n",
    "            return \"POSITIVE\"\n",
    "        else:\n",
    "            return \"NEGATIVE\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's now train the model,\n",
    "### We'll reserve last 1000 reviews for test and we'll train with remaining.\n",
    "#### the \"*3\" is basically the epoch.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy : 89.4 Trained percent : 100.                                                                                                       "
     ]
    }
   ],
   "source": [
    "nt = SentiNetwork(reviews[:-1000],labels[:-1000], alpha=0.1)\n",
    "nt.train(reviews[:-1000]*3,labels[:-1000]*3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Now testing the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Accuracy: 85.5 Completion % : 100. "
     ]
    }
   ],
   "source": [
    "nt.test(reviews[-1000:],labels[-1000:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now let's test our network with some random reviews.\n",
    "### For this, I have grabbed a snippet of the review for The Wolf of Wall Street with a rating of 9/10 from IMDB.Let's see what the network predicts:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'POSITIVE'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "st=\"\"\"Scorsese has always managed to elicit astounding performances from his actors, and his fifth collaboration with Leonardo DiCaprio results in one of the most charismatic, despicable, offensive and captivating characters to ever appear on screen. As financial bad boy Belfort, DiCaprio swaggers from scene to scene ingesting eye popping amounts of narcotics, groping and fondling nearly every female within reach, and spouting more profanity in three hours than an entire season of The Sopranos. Belfort is the kind of person that any sane person would detest in real life, but thanks to Scorses and DiCaprio, we cant take our eyes off him.\n",
    "\"\"\"\n",
    "nt.run(st)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### It predicts correctly. Now let's check with a snippet of review with rating 3/10:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'NEGATIVE'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "st2=\"\"\"The plot is nothing original, the rise and fall of a powerful man, have we not seen that so many times before ? Similar movies include Blow, Scarface and even the Spanish film starring Javier Bardem called Golden Balls. Overlong run time filled with lame rubbish over the top sex and drug scenes that seemed to take forever. I really cannot find anything good about this film, except a few laugh out loud scenes and a promising start that took a strong curve downwards as the movie progressed, 2 1/2 hours of torture.\"\"\"\n",
    "nt.run(st2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### and it predicts correctly again."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
